{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "oivqcgYiYcrm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeyiB5ftTdWr"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers==4.36.0\n",
        "!pip install datasets\n",
        "!pip install pillow\n",
        "!pip install opencv-python\n",
        "!pip install torch torchvision\n",
        "!pip install evaluate\n",
        "!pip install jiwer\n",
        "!pip install accelerate -U\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Libraries"
      ],
      "metadata": {
        "id": "bHfjGDUyYhoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    TrOCRProcessor,\n",
        "    VisionEncoderDecoderModel,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    default_data_collator\n",
        ")\n",
        "from datasets import load_metric\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ],
      "metadata": {
        "id": "PM8-O7-AYRUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Processing Functions"
      ],
      "metadata": {
        "id": "vKS9YfKBYmWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrescriptionImagePreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessing pipeline for prescription images following the methodology\n",
        "    from Ponnuru et al. (2024) as described in the thesis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, target_size=(384, 384)):\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def preprocess_image(self, image_path):\n",
        "        \"\"\"\n",
        "        Apply preprocessing steps:\n",
        "        1. Resize with INTER_LINEAR interpolation\n",
        "        2. Convert to grayscale\n",
        "        3. Adaptive thresholding (ADAPTIVE_THRESH_GAUSSIAN_C, block_size=61, C=11)\n",
        "        \"\"\"\n",
        "        # Read image\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            raise ValueError(f\"Cannot read image: {image_path}\")\n",
        "\n",
        "        # Step 1: Resize with INTER_LINEAR interpolation\n",
        "        img_resized = cv2.resize(img, self.target_size, interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # Step 2: Convert to grayscale\n",
        "        img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Step 3: Adaptive thresholding\n",
        "        # Parameters: block_size=61, C=11 (from thesis methodology)\n",
        "        img_thresh = cv2.adaptiveThreshold(\n",
        "            img_gray,\n",
        "            255,\n",
        "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "            cv2.THRESH_BINARY,\n",
        "            blockSize=61,\n",
        "            C=11\n",
        "        )\n",
        "\n",
        "        # Convert to PIL Image for TrOCR processor\n",
        "        return Image.fromarray(img_thresh)\n",
        "\n",
        "    def augment_image(self, image):\n",
        "        \"\"\"\n",
        "        Data augmentation techniques from Ali et al. (2024):\n",
        "        - Brightness adjustment\n",
        "        - Contrast normalization\n",
        "        - Translation\n",
        "        - Minor shearing\n",
        "        - Elastic transformation\n",
        "        - Gaussian noise\n",
        "        - Cropping with padding\n",
        "        \"\"\"\n",
        "        img_array = np.array(image)\n",
        "\n",
        "        # Random brightness adjustment\n",
        "        if np.random.random() > 0.5:\n",
        "            brightness_factor = np.random.uniform(0.8, 1.2)\n",
        "            img_array = np.clip(img_array * brightness_factor, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # Contrast normalization\n",
        "        if np.random.random() > 0.5:\n",
        "            alpha = np.random.uniform(0.9, 1.1)\n",
        "            img_array = cv2.convertScaleAbs(img_array, alpha=alpha, beta=0)\n",
        "\n",
        "        # Translation (slight shift)\n",
        "        if np.random.random() > 0.5:\n",
        "            tx, ty = np.random.randint(-20, 20, 2)\n",
        "            M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
        "            img_array = cv2.warpAffine(img_array, M, (img_array.shape[1], img_array.shape[0]))\n",
        "\n",
        "        # Gaussian noise\n",
        "        if np.random.random() > 0.5:\n",
        "            noise = np.random.normal(0, 5, img_array.shape)\n",
        "            img_array = np.clip(img_array + noise, 0, 255).astype(np.uint8)\n",
        "\n",
        "        return Image.fromarray(img_array)\n",
        "\n",
        "# Test the preprocessor\n",
        "preprocessor = PrescriptionImagePreprocessor()\n",
        "\n"
      ],
      "metadata": {
        "id": "U3jVgIsOYqvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset\n"
      ],
      "metadata": {
        "id": "DrKvJb7GeZzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrescriptionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for prescription images and transcriptions.\n",
        "\n",
        "    Expected data format:\n",
        "    - images/: folder containing prescription images\n",
        "    - annotations.csv: CSV with columns ['image_id', 'transcription']\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_df, image_dir, processor, preprocessor, augment=False):\n",
        "        self.data_df = data_df\n",
        "        self.image_dir = image_dir\n",
        "        self.processor = processor\n",
        "        self.preprocessor = preprocessor\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image path and transcription\n",
        "        row = self.data_df.iloc[idx]\n",
        "        image_path = os.path.join(self.image_dir, row['image_id'])\n",
        "        transcription = row['transcription']\n",
        "\n",
        "        # Preprocess image\n",
        "        image = self.preprocessor.preprocess_image(image_path)\n",
        "\n",
        "        # Apply augmentation during training\n",
        "        if self.augment:\n",
        "            image = self.preprocessor.augment_image(image)\n",
        "\n",
        "        # Process with TrOCR processor\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.squeeze()\n",
        "\n",
        "        # Tokenize transcription\n",
        "        labels = self.processor.tokenizer(\n",
        "            transcription,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128,\n",
        "            truncation=True\n",
        "        ).input_ids\n",
        "\n",
        "        # Replace padding token id with -100 for loss calculation\n",
        "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"labels\": torch.tensor(labels)\n",
        "        }\n",
        "\n",
        "print(\"Dataset class created\")\n"
      ],
      "metadata": {
        "id": "XDolJ8X_ebGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation and 5-Fold Cross-Validation Setup"
      ],
      "metadata": {
        "id": "y-zI6rfAefex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_splits(annotations_csv, n_folds=5, test_size=0.2, random_seed=42):\n",
        "    \"\"\"\n",
        "    Implements the two-phase data splitting strategy:\n",
        "    1. Hold out 20% for final test set\n",
        "    2. Use remaining 80% for 5-fold cross-validation\n",
        "\n",
        "    This follows the methodology in Section 3.5 of the thesis.\n",
        "    \"\"\"\n",
        "    # Load annotations\n",
        "    df = pd.read_csv(annotations_csv)\n",
        "\n",
        "    # First split: 80% train/val, 20% test\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    train_val_df, test_df = train_test_split(\n",
        "        df,\n",
        "        test_size=test_size,\n",
        "        random_state=random_seed\n",
        "    )\n",
        "\n",
        "    # Setup 5-fold cross-validation on the train_val set\n",
        "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=random_seed)\n",
        "\n",
        "    # Save test set\n",
        "    test_df.to_csv('test_set.csv', index=False)\n",
        "\n",
        "    print(f\"ðŸ“Š Data Split Summary:\")\n",
        "    print(f\"   Total samples: {len(df)}\")\n",
        "    print(f\"   Train/Val samples (80%): {len(train_val_df)}\")\n",
        "    print(f\"   Test samples (20%): {len(test_df)}\")\n",
        "    print(f\"   Cross-validation folds: {n_folds}\")\n",
        "\n",
        "    return train_val_df, test_df, kfold\n",
        "\n",
        "# Example usage (you'll need to provide your actual CSV file)\n",
        "# train_val_df, test_df, kfold = prepare_data_splits('annotations.csv')\n",
        "print(\"Data splitting function ready\")\n"
      ],
      "metadata": {
        "id": "NY4EM5CVeh8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Initialization"
      ],
      "metadata": {
        "id": "pgUY2CbUeojI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_trocr_model():\n",
        "    \"\"\"\n",
        "    Initialize TrOCR-Base-Handwritten model and processor.\n",
        "\n",
        "    Model: microsoft/trocr-base-handwritten (pre-trained)\n",
        "    This follows Table 3.1 in the thesis methodology.\n",
        "    \"\"\"\n",
        "    model_name = \"microsoft/trocr-base-handwritten\"\n",
        "\n",
        "    # Load processor (handles image preprocessing and tokenization)\n",
        "    processor = TrOCRProcessor.from_pretrained(model_name)\n",
        "\n",
        "    # Load model\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
        "\n",
        "    # Set special tokens\n",
        "    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "    model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "    model.config.vocab_size = model.config.decoder.vocab_size\n",
        "\n",
        "    # Enable gradient checkpointing to save memory\n",
        "    model.config.use_cache = False\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    print(f\"âœ… Model loaded: {model_name}\")\n",
        "    print(f\"   Encoder: {model.config.encoder.model_type}\")\n",
        "    print(f\"   Decoder: {model.config.decoder.model_type}\")\n",
        "    print(f\"   Vocab size: {model.config.vocab_size}\")\n",
        "\n",
        "    return model, processor\n",
        "\n",
        "# Initialize model and processor\n",
        "model, processor = initialize_trocr_model()\n"
      ],
      "metadata": {
        "id": "ja-UbGGCepXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation Metrics (CER & WER)"
      ],
      "metadata": {
        "id": "KQc99sThesLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jiwer import wer, cer\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"\n",
        "    Compute Character Error Rate (CER) and Word Error Rate (WER).\n",
        "\n",
        "    CER: Measures character-level transcription accuracy using Levenshtein distance\n",
        "    WER: Measures word-level transcription accuracy\n",
        "\n",
        "    As described in Section 3.6.1 of the thesis.\n",
        "    \"\"\"\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    # Replace -100 in labels (used for padding)\n",
        "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    # Decode predictions and labels\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Calculate CER and WER\n",
        "    cer_score = cer(label_str, pred_str)\n",
        "    wer_score = wer(label_str, pred_str)\n",
        "\n",
        "    return {\n",
        "        \"cer\": cer_score,\n",
        "        \"wer\": wer_score\n",
        "    }\n",
        "\n",
        "print(\"Evaluation metrics (CER & WER) configured\")\n"
      ],
      "metadata": {
        "id": "ZzgrfecSetFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Configuration"
      ],
      "metadata": {
        "id": "7k1gcY0SewQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_args(output_dir, num_train_epochs=15):\n",
        "    \"\"\"\n",
        "    Training hyperparameters from Table 3.1 of the thesis:\n",
        "    - Optimizer: AdamW\n",
        "    - Learning rate: 2e-5\n",
        "    - Batch size: 16\n",
        "    - Weight decay: 0.01\n",
        "    - Epochs: 10-15 with early stopping\n",
        "    - Scheduler: Linear with warm-up\n",
        "    - Gradient clipping: 1.0\n",
        "    \"\"\"\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "\n",
        "        # Training hyperparameters from Table 3.1\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        learning_rate=2e-5,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        weight_decay=0.01,\n",
        "        max_grad_norm=1.0,  # Gradient clipping\n",
        "\n",
        "        # AdamW optimizer (default in Trainer)\n",
        "        optim=\"adamw_torch\",\n",
        "\n",
        "        # Learning rate scheduler: Linear with warm-up\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        warmup_steps=500,\n",
        "\n",
        "        # Early stopping configuration\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"cer\",\n",
        "        greater_is_better=False,  # Lower CER is better\n",
        "\n",
        "        # Evaluation and saving\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=500,\n",
        "        save_total_limit=3,  # Keep only 3 best checkpoints\n",
        "\n",
        "        # Logging\n",
        "        logging_steps=100,\n",
        "        logging_dir=f\"{output_dir}/logs\",\n",
        "\n",
        "        # Other settings\n",
        "        predict_with_generate=True,\n",
        "        fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
        "        dataloader_num_workers=2,\n",
        "        remove_unused_columns=False,\n",
        "\n",
        "        # For reproducibility\n",
        "        seed=42,\n",
        "    )\n",
        "\n",
        "    return training_args\n",
        "\n",
        "print(\"Training arguments configured with thesis hyperparameters\")\n"
      ],
      "metadata": {
        "id": "mkMX6y5EexfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Loop with 5-Fold Cross-Validation"
      ],
      "metadata": {
        "id": "qULDbEoae2FJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_cross_validation(\n",
        "    train_val_df,\n",
        "    kfold,\n",
        "    image_dir,\n",
        "    processor,\n",
        "    preprocessor,\n",
        "    base_output_dir=\"./trocr_cv_results\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Train TrOCR using 5-fold cross-validation as described in Section 3.5.\n",
        "\n",
        "    For each fold:\n",
        "    - 4 folds for training\n",
        "    - 1 fold for validation\n",
        "    - Track CER and WER metrics\n",
        "    \"\"\"\n",
        "    fold_results = []\n",
        "\n",
        "    for fold_idx, (train_indices, val_indices) in enumerate(kfold.split(train_val_df)):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"ðŸ“‚ FOLD {fold_idx + 1}/{kfold.n_splits}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        # Split data for this fold\n",
        "        train_fold_df = train_val_df.iloc[train_indices].reset_index(drop=True)\n",
        "        val_fold_df = train_val_df.iloc[val_indices].reset_index(drop=True)\n",
        "\n",
        "        print(f\"Training samples: {len(train_fold_df)}\")\n",
        "        print(f\"Validation samples: {len(val_fold_df)}\")\n",
        "\n",
        "        # Create datasets\n",
        "        train_dataset = PrescriptionDataset(\n",
        "            train_fold_df, image_dir, processor, preprocessor, augment=True\n",
        "        )\n",
        "        val_dataset = PrescriptionDataset(\n",
        "            val_fold_df, image_dir, processor, preprocessor, augment=False\n",
        "        )\n",
        "\n",
        "        # Initialize fresh model for this fold\n",
        "        model, _ = initialize_trocr_model()\n",
        "\n",
        "        # Training arguments\n",
        "        output_dir = f\"{base_output_dir}/fold_{fold_idx + 1}\"\n",
        "        training_args = get_training_args(output_dir)\n",
        "\n",
        "        # Early stopping callback (stop if no improvement for 3 evaluations)\n",
        "        from transformers import EarlyStoppingCallback\n",
        "        early_stopping = EarlyStoppingCallback(\n",
        "            early_stopping_patience=3,\n",
        "            early_stopping_threshold=0.0\n",
        "        )\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = Seq2SeqTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "            data_collator=default_data_collator,\n",
        "            compute_metrics=compute_metrics,\n",
        "            callbacks=[early_stopping]\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        print(f\"\\nðŸš€ Starting training for Fold {fold_idx + 1}...\\n\")\n",
        "        train_result = trainer.train()\n",
        "\n",
        "        # Evaluate\n",
        "        print(f\"\\nðŸ“Š Evaluating Fold {fold_idx + 1}...\\n\")\n",
        "        eval_result = trainer.evaluate()\n",
        "\n",
        "        # Store results\n",
        "        fold_results.append({\n",
        "            'fold': fold_idx + 1,\n",
        "            'train_loss': train_result.training_loss,\n",
        "            'eval_cer': eval_result['eval_cer'],\n",
        "            'eval_wer': eval_result['eval_wer']\n",
        "        })\n",
        "\n",
        "        # Save model\n",
        "        trainer.save_model(f\"{output_dir}/best_model\")\n",
        "\n",
        "        print(f\"\\nâœ… Fold {fold_idx + 1} completed!\")\n",
        "        print(f\"   CER: {eval_result['eval_cer']:.4f}\")\n",
        "        print(f\"   WER: {eval_result['eval_wer']:.4f}\")\n",
        "\n",
        "    # Calculate average metrics across folds\n",
        "    avg_cer = np.mean([r['eval_cer'] for r in fold_results])\n",
        "    avg_wer = np.mean([r['eval_wer'] for r in fold_results])\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ðŸ“ˆ CROSS-VALIDATION SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Average CER: {avg_cer:.4f}\")\n",
        "    print(f\"Average WER: {avg_wer:.4f}\")\n",
        "\n",
        "    # Save results\n",
        "    results_df = pd.DataFrame(fold_results)\n",
        "    results_df.to_csv(f\"{base_output_dir}/cv_results.csv\", index=False)\n",
        "\n",
        "    return fold_results\n",
        "\n",
        "print(\"Cross-validation training function set\")\n"
      ],
      "metadata": {
        "id": "TSYFwftVe3xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Model Training and Testing"
      ],
      "metadata": {
        "id": "TxvADw_6e6_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_final_model(train_val_df, image_dir, processor, preprocessor, output_dir=\"./trocr_final\"):\n",
        "    \"\"\"\n",
        "    After cross-validation, train final model on entire train/val set.\n",
        "    Then evaluate on held-out test set.\n",
        "    \"\"\"\n",
        "    print(\"\\nðŸŽ¯ Training final model on complete train/val set...\\n\")\n",
        "\n",
        "    # Create dataset\n",
        "    train_dataset = PrescriptionDataset(\n",
        "        train_val_df, image_dir, processor, preprocessor, augment=True\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model, _ = initialize_trocr_model()\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = get_training_args(output_dir, num_train_epochs=15)\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        data_collator=default_data_collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Save final model\n",
        "    trainer.save_model(f\"{output_dir}/final_model\")\n",
        "    processor.save_pretrained(f\"{output_dir}/final_model\")\n",
        "\n",
        "    print(\"âœ… Final model trained and saved!\")\n",
        "\n",
        "    return trainer\n",
        "\n",
        "def evaluate_on_test_set(trainer, test_df, image_dir, processor, preprocessor):\n",
        "    \"\"\"\n",
        "    Evaluate the final model on the held-out 20% test set.\n",
        "    \"\"\"\n",
        "    print(\"\\nðŸ§ª Evaluating on held-out test set...\\n\")\n",
        "\n",
        "    # Create test dataset (no augmentation)\n",
        "    test_dataset = PrescriptionDataset(\n",
        "        test_df, image_dir, processor, preprocessor, augment=False\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    test_results = trainer.evaluate(test_dataset)\n",
        "\n",
        "    print(\"\\nðŸ“Š TEST SET RESULTS:\")\n",
        "    print(f\"   CER: {test_results['eval_cer']:.4f}\")\n",
        "    print(f\"   WER: {test_results['eval_wer']:.4f}\")\n",
        "\n",
        "    return test_results\n",
        "\n",
        "print(\"Final training and testing functions set\")\n"
      ],
      "metadata": {
        "id": "vAO_-yYue74g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference Function"
      ],
      "metadata": {
        "id": "RPrmk2RQe9tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_prescription(image_path, model, processor, preprocessor):\n",
        "    \"\"\"\n",
        "    Perform inference on a single prescription image.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to prescription image\n",
        "        model: Trained TrOCR model\n",
        "        processor: TrOCR processor\n",
        "        preprocessor: Image preprocessor\n",
        "\n",
        "    Returns:\n",
        "        Transcribed text string\n",
        "    \"\"\"\n",
        "    # Preprocess image\n",
        "    image = preprocessor.preprocess_image(image_path)\n",
        "\n",
        "    # Convert to pixel values\n",
        "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "    # Move to same device as model\n",
        "    device = next(model.parameters()).device\n",
        "    pixel_values = pixel_values.to(device)\n",
        "\n",
        "    # Generate transcription\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(pixel_values, max_length=128)\n",
        "\n",
        "    # Decode\n",
        "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    return transcription\n",
        "\n",
        "# Example usage:\n",
        "# transcription = predict_prescription(\"path/to/prescription.jpg\", model, processor, preprocessor)\n",
        "# print(f\"Transcription: {transcription}\")\n",
        "\n",
        "print(\"Inference function set\")\n"
      ],
      "metadata": {
        "id": "Dte71RS2e_jQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}